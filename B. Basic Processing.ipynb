{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e56dee-cd00-414e-8445-d6296ec8bce2",
   "metadata": {},
   "source": [
    "# B. Basic Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142f48cf-6c17-4aee-9a40-4007a7ef2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5663b649-80dd-4a0d-986e-17ac2021000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = 'C:/Users/user/Desktop/24Spring_ETA_Project/Dataset'\n",
    "data_prefix = 'Star Wars- Episode '\n",
    "output_dir = 'C:/Users/user/Desktop/24Spring_ETA_Project/Output'\n",
    "path_prefix = 'starwars-combo'\n",
    "\n",
    "ep1 = f\"{data_home}/{data_prefix}I - The Phantom Menace (1999).rtf\"\n",
    "ep2 = f\"{data_home}/{data_prefix}II - Attack of the Clones (2002).rtf\"\n",
    "ep3 = f\"{data_home}/{data_prefix}III - Revenge of the Sith (2005).rtf\"\n",
    "ep4 = f\"{data_home}/{data_prefix}IV - A New Hope (1977).rtf\"\n",
    "ep5 = f\"{data_home}/{data_prefix}V - The Empire Strikes Back (1980).rtf\"\n",
    "ep6 = f\"{data_home}/{data_prefix}VI - Return of the Jedi (1983).rtf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999ad0af-aba2-4f6b-8e27-dec584acb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e610e5-82c5-42f3-ae1a-8788a693a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chunks:\n",
    "    def __init__(self, episode):\n",
    "        self.episode = episode\n",
    "\n",
    "    def getlines(self):\n",
    "        LINES = pd.DataFrame(open(self.episode, 'r', encoding='utf-8-sig').readlines(), columns=['line_str'])\n",
    "        LINES.index.name = 'line_num'\n",
    "        LINES.line_str = LINES.line_str.str.replace(r'\\\\\\n+', ' ', regex=True).str.strip()\n",
    "        if self.episode == ep1: #start from actual content to the end\n",
    "            LINES = LINES.loc[11 :]\n",
    "        elif self.episode == ep2:\n",
    "            LINES = LINES.loc[10 :]\n",
    "        elif self.episode == ep3:\n",
    "            LINES = LINES.loc[7 :]\n",
    "        elif self.episode == ep4:\n",
    "            LINES = LINES.loc[12 :]\n",
    "        elif self.episode == ep5:\n",
    "            LINES = LINES.loc[11 :]\n",
    "        elif self.episode == ep6:\n",
    "            LINES = LINES.loc[11 :]\n",
    "        return LINES\n",
    "\n",
    "    \n",
    "    def chap(self, LINES):\n",
    "        if self.episode == ep4:\n",
    "            chap_pat = r\"^\\s*(INTERIOR\\:|EXTERIOR\\:)\"\n",
    "        elif self.episode == ep3:\n",
    "            chap_pat = r\"^\\s*\\d+\\s+(INT\\.|EXT\\.)\"\n",
    "        elif self.episode == ep6:\n",
    "            chap_pat = r\"^\\s*\\d+\\s+(INT|EXT)\"\n",
    "            # 11(tab)INT JABBA'S THRONE ROOM(tab)(tab)(tab)(tab)11\n",
    "        else:\n",
    "            chap_pat = r\"^\\s*(INT\\.|EXT\\.)\"\n",
    "\n",
    "        chap_lines = LINES.line_str.str.match(chap_pat, case=False)\n",
    "        LINES.loc[chap_lines]\n",
    "        LINES.loc[chap_lines, 'chap_num'] = [i+1 for i in range(LINES.loc[chap_lines].shape[0])]\n",
    "        LINES.loc[chap_lines]\n",
    "    \n",
    "        LINES.chap_num = LINES.chap_num.ffill()\n",
    "        LINES['chap_num'] = LINES['chap_num'].fillna(1) # Don't Remove everything before Chapter 1\n",
    "    \n",
    "        OHCO[:1]\n",
    "        CHAPS = LINES.groupby(OHCO[:1])\\\n",
    "            .line_str.apply(lambda x: '\\n'.join(x))\\\n",
    "            .to_frame('chap_str')\n",
    "        CHAPS['chap_str'] = CHAPS.chap_str.str.strip()\n",
    "        return CHAPS\n",
    "\n",
    "    def par(self, CHAPS):\n",
    "        para_pat = r'\\n\\n+'\n",
    "    \n",
    "        PARAS = CHAPS['chap_str'].str.split(para_pat, expand=True).stack()\\\n",
    "            .to_frame('para_str').sort_index()\n",
    "        PARAS.index.names = OHCO[:2]\n",
    "        PARAS.head()\n",
    "    \n",
    "        PARAS['para_str'] = PARAS['para_str'].str.replace(r'\\n', ' ', regex=True)\n",
    "        PARAS['para_str'] = PARAS['para_str'].str.strip()\n",
    "        PARAS = PARAS[~PARAS['para_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        return PARAS\n",
    "    \n",
    "    def sent(self, PARAS):\n",
    "        sent_pat = r'[.?!;:]+'\n",
    "        SENTS = PARAS['para_str'].str.split(sent_pat, expand=True).stack()\\\n",
    "            .to_frame('sent_str')\n",
    "        SENTS.index.names = OHCO[:3]\n",
    "    \n",
    "        SENTS = SENTS[~SENTS['sent_str'].str.match(r'^\\s*$')] # Remove empty paragraphs\n",
    "        SENTS.sent_str = SENTS.sent_str.str.strip() # CRUCIAL TO REMOVE BLANK TOKENS\n",
    "    \n",
    "        return SENTS\n",
    "    \n",
    "    def token(self, SENTS):\n",
    "        token_pat = r\"[\\s',-]+\"\n",
    "        # SENTS_new = SENTS['sent_str'].str.replace('-', '', regex=True)\n",
    "        TOKENS = SENTS['sent_str'].str.split(token_pat, expand=True).stack()\\\n",
    "            .to_frame('token_str')\n",
    "        \n",
    "        TOKENS.index.names = OHCO[:4]\n",
    "        TOKENS['term_str'] = TOKENS.token_str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "\n",
    "        TOKENS = TOKENS[~TOKENS['term_str'].str.isnumeric()]\n",
    "        \n",
    "        return TOKENS\n",
    "    \n",
    "    def vocab(self, TOKENS):\n",
    "        VOCAB = TOKENS.term_str.value_counts().to_frame('n').reset_index().rename(columns={'index':'term_str'})\n",
    "        VOCAB.index.name = 'term_id'\n",
    "        return VOCAB\n",
    "\n",
    "    def process(self):\n",
    "        episode_chunk = chunks(self.episode)\n",
    "        ep_lines = episode_chunk.getlines()\n",
    "        ep_chaps = episode_chunk.chap(ep_lines)\n",
    "        ep_pars = episode_chunk.par(ep_chaps)\n",
    "        ep_sents = episode_chunk.sent(ep_pars)\n",
    "        ep_tokens = episode_chunk.token(ep_sents)\n",
    "        ep_vocab = episode_chunk.vocab(ep_tokens)\n",
    "        return {\n",
    "            'lines': ep_lines,\n",
    "            'chapters': ep_chaps,\n",
    "            'paragraphs': ep_pars,\n",
    "            'sentences': ep_sents,\n",
    "            'tokens': ep_tokens,\n",
    "            'vocabulary': ep_vocab\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b3c765-c451-449b-b088-b3082072279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_files = [ep1, ep2, ep3, ep4, ep5, ep6]\n",
    "\n",
    "# Dictionary to hold all processed data by episode\n",
    "episode_data = {}\n",
    "\n",
    "# Loop through each episode file\n",
    "for i, episode_file in enumerate(episode_files, start=1):\n",
    "    episode_chunk = chunks(episode_file)\n",
    "    processed = episode_chunk.process()  # This should return a dictionary of all stages\n",
    "    \n",
    "    episode_data[f'ep{i}'] = {\n",
    "        'lines': processed[\"lines\"],\n",
    "        'chapters': processed[\"chapters\"],\n",
    "        'paragraphs': processed[\"paragraphs\"],\n",
    "        'sentences': processed[\"sentences\"],\n",
    "        'tokens': processed[\"tokens\"],\n",
    "        'vocabulary': processed[\"vocabulary\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f89488-a465-4920-8bd2-8c7a0b33ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep1_token = episode_data['ep1']['tokens']\n",
    "ep1_token.to_csv(f\"{output_dir}/basic/ep1-TOKEN.csv\")\n",
    "ep2_token = episode_data['ep2']['tokens']\n",
    "ep2_token.to_csv(f\"{output_dir}/basic/ep2-TOKEN.csv\")\n",
    "ep3_token = episode_data['ep3']['tokens']\n",
    "ep3_token.to_csv(f\"{output_dir}/basic/ep3-TOKEN.csv\")\n",
    "ep4_token = episode_data['ep4']['tokens']\n",
    "ep4_token.to_csv(f\"{output_dir}/basic/ep4-TOKEN.csv\")\n",
    "ep5_token = episode_data['ep5']['tokens']\n",
    "ep5_token.to_csv(f\"{output_dir}/basic/ep5-TOKEN.csv\")\n",
    "ep6_token = episode_data['ep6']['tokens']\n",
    "ep6_token.to_csv(f\"{output_dir}/basic/ep6-TOKEN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a723b-b678-4c30-aec2-298d3a9f540a",
   "metadata": {},
   "source": [
    "## Save tokens as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ff0f3a-b7da-4efb-9ae4-c55ee44ad185",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep1_token = episode_data['ep1']['tokens']\n",
    "ep1_token.to_csv(f\"{output_dir}/basic/ep1-TOKEN.csv\")\n",
    "ep2_token = episode_data['ep2']['tokens']\n",
    "ep2_token.to_csv(f\"{output_dir}/basic/ep2-TOKEN.csv\")\n",
    "ep3_token = episode_data['ep3']['tokens']\n",
    "ep3_token.to_csv(f\"{output_dir}/basic/ep3-TOKEN.csv\")\n",
    "ep4_token = episode_data['ep4']['tokens']\n",
    "ep4_token.to_csv(f\"{output_dir}/basic/ep4-TOKEN.csv\")\n",
    "ep5_token = episode_data['ep5']['tokens']\n",
    "ep5_token.to_csv(f\"{output_dir}/basic/ep5-TOKEN.csv\")\n",
    "ep6_token = episode_data['ep6']['tokens']\n",
    "ep6_token.to_csv(f\"{output_dir}/basic/ep6-TOKEN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dfa96-b147-4a51-8aec-3b88bb6cfd28",
   "metadata": {},
   "source": [
    "## Save vocabs as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1348ebf9-9470-4a88-b50b-3144a2159b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep1_vocab = episode_data['ep1']['vocabulary']\n",
    "ep1_vocab.to_csv(f\"{output_dir}/basic/ep1-VOCAB.csv\")\n",
    "ep2_vocab = episode_data['ep2']['vocabulary']\n",
    "ep2_vocab.to_csv(f\"{output_dir}/basic/ep2-VOCAB.csv\")\n",
    "ep3_vocab = episode_data['ep3']['vocabulary']\n",
    "ep3_vocab.to_csv(f\"{output_dir}/basic/ep3-VOCAB.csv\")\n",
    "ep4_vocab = episode_data['ep4']['vocabulary']\n",
    "ep4_vocab.to_csv(f\"{output_dir}/basic/ep4-VOCAB.csv\")\n",
    "ep5_vocab = episode_data['ep5']['vocabulary']\n",
    "ep5_vocab.to_csv(f\"{output_dir}/basic/ep5-VOCAB.csv\")\n",
    "ep6_vocab = episode_data['ep6']['vocabulary']\n",
    "ep6_vocab.to_csv(f\"{output_dir}/basic/ep6-VOCAB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628fbc1-b3e0-4687-850a-0eb7852d17d2",
   "metadata": {},
   "source": [
    "## Save sent as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb0421a-9d51-4697-ab63-cb3d36cd82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep1_sent = episode_data['ep1']['sentences']\n",
    "ep1_sent.to_csv(f\"{output_dir}/basic/ep1-SENT.csv\")\n",
    "ep2_sent = episode_data['ep2']['sentences']\n",
    "ep2_sent.to_csv(f\"{output_dir}/basic/ep2-SENT.csv\")\n",
    "ep3_sent = episode_data['ep3']['sentences']\n",
    "ep3_sent.to_csv(f\"{output_dir}/basic/ep3-SENT.csv\")\n",
    "ep4_sent = episode_data['ep4']['sentences']\n",
    "ep4_sent.to_csv(f\"{output_dir}/basic/ep4-SENT.csv\")\n",
    "ep5_sent = episode_data['ep5']['sentences']\n",
    "ep5_sent.to_csv(f\"{output_dir}/basic/ep5-SENT.csv\")\n",
    "ep6_sent = episode_data['ep6']['sentences']\n",
    "ep6_sent.to_csv(f\"{output_dir}/basic/ep6-SENT.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
